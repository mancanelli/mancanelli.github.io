---
layout: default
title: 'PHYDI: Initializing Parameterized Hypercomplex <span class="nobr">Neural Networks</span> as Identity Functions'
authors: <span class="nobr">Matteo Mancanelli</span>, <span class="nobr">Eleonora Grassucci</span>, <span class="nobr">Aurelio Uncini</span>, <span class="nobr">Danilo Comminiello</span>
publication: IEEE International Workshop on Machine Learning for <span class="nobr">Signal Processing</span> (MLSP 2023)
month: Sept.
year: 2023
type: conference
paper: PHYDI.pdf
doi: https://ieeexplore.ieee.org/document/10285926
preprint: https://arxiv.org/abs/2310.07612
poster: PHYDI_poster.pdf
code: https://github.com/ispamm/PHYDI
abstract: "Neural models based on hypercomplex algebra systems are growing and prolificating for a plethora of applications, ranging from computer 
  vision to natural language processing. Hand in hand with their adoption, parameterized hypercomplex neural networks (PHNNs) are growing in size and no techniques have been adopted so far to control their convergence at a large scale. In this paper, we study PHNNs convergence and propose parameterized hypercomplex identity initialization (PHYDI), a method to improve their convergence at different scales, leading to more robust performance when the number of layers scales up, while also reaching the same performance with fewer iterations. We show the effectiveness of this approach in different benchmarks and with common PHNNs with ResNets- and Transformer-based architecture."
bibtex: "@inproceedings{mancanelli2023MLSP,<br/>
  &emsp;author={Mancanelli, Matteo and Grassucci, Eleonora and Uncini, Aurelio and Comminiello, Danilo},<br/>
  &emsp;booktitle={2023 IEEE 33rd International Workshop on Machine Learning for Signal Processing (MLSP)},<br/>
  &emsp;title={{PHYDI: I}nitializing Parameterized Hypercomplex Neural Networks as Identity Functions},<br/>
  &emsp;year={2023},<br/>
  &emsp;organization={IEEE},<br/>
  &emsp;pages={1--6},<br/>
  &emsp;doi={10.1109/MLSP55844.2023.10285926}<br/>
&nbsp;}"
ack: <span class="nobr">Top 5% Outstanding Paper</span>
---